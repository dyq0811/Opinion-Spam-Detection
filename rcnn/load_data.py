"""Load the review data."""
import torch
import numpy as np
import data_helper as dh
import utility
import pickle

# read the dictionary and the word embeddings back from pickle
pkl_file = open('word2vec.pkl', 'rb')
dictionary, embeddings = pickle.load(pkl_file)
pkl_file.close()

# read each review file
neg_dec_data, neg_dec_label = dh.load_data("neg_deceptive")
neg_true_data, neg_true_label = dh.load_data("neg_truthful")
pos_dec_data, pos_dec_label = dh.load_data("pos_deceptive")
pos_true_data, pos_true_label = dh.load_data("pos_truthful")

test_prop = 1/10

def load():
    """Load all review data and labels"""
    dec_data = neg_dec_data + pos_dec_data
    true_data = neg_true_data + pos_true_data

    dec_label = neg_dec_label + pos_dec_label
    true_label = neg_true_label + pos_true_label
    
    # Combine data
    data = dec_data + true_data
    label = []
    for l in dec_label:
        label.append(l[0])
    for l in true_label:
        label.append(l[0])

    data, label = utility.shuffle(data, label)

    return data, label

def load_pos():
    """Load only positive review data and labels"""
    data = pos_dec_data + pos_true_data
    label = [0] * len(pos_dec_data) + [1] * len(pos_true_data)
    data, label = utility.shuffle(data, label)
    return data, label

def load_neg():
    """Load only negative review data and labels"""
    data = neg_dec_data + neg_true_data
    label = [0] * len(neg_dec_data) + [1] * len(neg_true_data)
    data, label = utility.shuffle(data, label)
    return data, label
    
def split_data(data, label, batch_size):
    """Split the data into batches of training data and 
    batches of testing data.
    Each batch is represented by a tensor of size 
    (batch_size, num_sequences, embedding_length).
    """
    permute = np.random.permutation(len(data))
    test_data = data[:int(len(data) * test_prop)]
    train_data = data[int(len(data) * test_prop):]
    test_label = label[:int(len(data) * test_prop)]
    train_label = label[int(len(data) * test_prop):]

    trainX = transform(train_data, batch_size)
    testX = transform(test_data, batch_size)

    testY = torch.tensor([test_label[i:i+batch_size] for i in range(0, len(test_label), batch_size)])
    trainY = torch.tensor([train_label[i:i+batch_size] for i in range(0, len(train_label), batch_size)])
    
    return trainX, trainY, testX, testY

def encode(words):
    """Turn a list of words into a tensor of shape 
    <len_words x 1 x 128>, where each word is a 
    128-dimensional vector generated by the word2vec network."""
    res = []
    for i in range(len(words)):
        word = words[i]
        if word in dictionary:
            embed = embeddings[dictionary[word]]
        else:
            embed = embeddings[dictionary["UNK"]]
        res.append(embed.tolist())
    return res

def pad(reviews):
    """Pad a list of reviews into equal length"""
    max_len = len(max(reviews, key=len))
    for r in reviews:
        for i in range(max_len - len(r)):
            r.append("UNK")
    return reviews

def transform(data, batch_size):
    """Transform the data into a list of vectors of size
    (batch_size, num_sequence, embedding_length).
    num_sequence is the length of the longest review in a
    batch and every other review is padded to that length.
    embedding_length is the dimension of the vector that represents 
    each sequence. In this case, this is the word2vec pretrained
    where the vector has 128 dimensions.
    """
    res = []
    for i in range(0, len(data), batch_size):
        reviews = data[i:i+batch_size]
        reviews = pad(reviews)
        cur_batch = []
        for r in reviews:
            cur_batch.append(encode(r))
        cur_batch = torch.Tensor(cur_batch)
        res.append(cur_batch)
    return res